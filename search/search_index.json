{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pawa IT Cloud Mastery Training","text":"Intelligent Foundations Oscar Limoke        Get introduced to cloud computing fundamentals, training objectives, and prepare for the comprehensive hands-on labs ahead.      Start Introduction GCP Migration Lab Eddie Ngugi        Learn to migrate on-premises applications to Google Cloud Platform. Set up GCP environment and configure Cloud SQL database infrastructure.      Start Migration Labs Automate Securely Basil Ndonga        Master DevOps practices and CI/CD pipelines on GCP. Build, deploy, and manage applications with automated workflows and security best practices.      Start DevOps Lab Data to Dashboards John Higi        Transform raw data into actionable insights using GCP's analytics and visualization tools. Learn to build compelling dashboards and reports.      Start Analytics Lab Build Your Agent Nelson Ameyo        Create intelligent virtual agents using Dialogflow CX. Build conversational AI that can understand, respond, and assist users effectively.      Build Virtual Agent"},{"location":"end-of-training/","title":"\ud83c\udf89 Congratulations! Training Session Complete","text":"<p>You've successfully completed the Cloud Mastery training session! This has been an intensive journey through modern cloud technologies, and you should be proud of what you've accomplished.</p> <p>Keep Building</p> <p>\"The best way to learn data engineering is to keep building data pipelines. Every dataset tells a story, and every pipeline you build makes you a better engineer.\"</p>"},{"location":"end-of-training/#youre-ready-for-the-real-world","title":"\ud83d\ude80 You're Ready for the Real World!","text":"<p>With the skills you've developed in this training session, you're well-prepared to tackle challenges in production environments.</p> <p>Keep coding, keep learning, and keep building amazing solutions!</p> \u2190 Previous: Virtual Agents Lab  Section 26 -  End of Training  Return to Home \u2192"},{"location":"introduction/","title":"Cloud Mastery Training Introduction","text":""},{"location":"introduction/#overview","title":"Overview","text":"<p>This comprehensive training introduces core Google Cloud concepts and demonstrates how strategic cloud adoption drives real business value through improved performance, AI capabilities, and cost optimization.</p>"},{"location":"introduction/#why-cloud-matters","title":"Why Cloud Matters","text":"<p>Moving to the cloud isn't just about technology\u2014it's about transformation:</p> <ul> <li>Pay-as-you-go model: Eliminate wasted capacity and large upfront investments</li> <li>Instant scalability: Scale resources up or down in minutes, not weeks</li> <li>Global reach: Built-in redundancy and high availability across regions</li> <li>Focus on innovation: Let Google manage infrastructure while you drive business value</li> </ul>"},{"location":"introduction/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>GCP Core Concepts - Understanding the fundamental building blocks</li> <li>Identity &amp; Access Management - Secure every identity and control all access</li> <li>Google Cloud CLI - Essential tools for cloud management</li> <li>Cost Optimization - Strategies to maximize your cloud investment</li> <li>Security Best Practices - Proactive threat management and compliance</li> </ul>"},{"location":"introduction/#the-stakes-are-high","title":"The Stakes Are High","text":"<p>Poor cloud implementation can lead to:</p> <ul> <li>Runaway costs from misconfigured resources</li> <li>Critical data breaches exposing valuable assets</li> <li>Compliance failures resulting in fines and legal action</li> <li>Lost customer trust that's impossible to recover</li> </ul>"},{"location":"introduction/#prerequisites","title":"Prerequisites","text":"<ul> <li>Active Google account</li> <li>Basic understanding of cloud computing concepts</li> <li>Commitment to following security best practices from day one</li> </ul>"},{"location":"introduction/#your-cloud-journey-starts-here","title":"Your Cloud Journey Starts Here","text":"<p>This training follows the Well-Architected Framework principles, ensuring you build on a solid foundation rather than rushing to implement without proper planning. You'll learn to be a \"Cloud Architect\" who designs resilient, secure, and cost-effective environments.</p>"},{"location":"introduction/#next-steps","title":"Next Steps","text":"<p>Ready to transform your approach to cloud? Let's begin with the GCP Migration Lab where theory meets practice.</p> \u2190 Previous: Home Introduction Next: GCP Migration Lab \u2192"},{"location":"data-analytics/data-analytics-lab/","title":"Data to Dashboards: The Analytics Lab","text":"<p>Welcome to the Data Analytics &amp; Visualization lab! In this section, you will build a complete, automated data pipeline on Google Cloud. This is a powerful, real-world pattern used to move data from operational databases into a data warehouse for analysis and reporting.</p>"},{"location":"data-analytics/data-analytics-lab/#lab-overview","title":"Lab Overview","text":"<p>You will build a pipeline that:</p> <ol> <li>Sets up a Destination: You'll start by creating a home for your data in BigQuery, Google's serverless data warehouse.</li> <li>Extracts &amp; Transforms Data: You'll write and deploy a Cloud Function that connects to a MySQL database, runs a SQL query to transform the data, and prepares it for loading.</li> <li>Automates the Pipeline: You'll use Cloud Scheduler to automatically run your function on a recurring schedule, ensuring your data warehouse stays up-to-date.</li> <li>Visualizes the Insights: Finally, you'll connect Looker Studio to your BigQuery data to build and customize an interactive dashboard.</li> </ol>"},{"location":"data-analytics/data-analytics-lab/#whats-next","title":"What's Next","text":"<p>Let's begin by setting up the foundation of our data pipeline: the BigQuery dataset and table.</p> \u2190 Previous: Accessing the Application Section 20 - Analytics Lab Next: Setup BigQuery \u2192"},{"location":"data-analytics/data-pipeline-create-function/","title":"2. Create the Cloud Function (ETL)","text":"<p>This Cloud Function is the engine of our pipeline. It will run a Python script to connect to a MySQL database, execute a query to denormalize and fetch data, and then load that data into our BigQuery table, overwriting the previous contents.</p>"},{"location":"data-analytics/data-pipeline-create-function/#prerequisites","title":"Prerequisites","text":"<ul> <li>Enabled APIs: Ensure Cloud Functions, Cloud Build, Artifact Registry, and BigQuery APIs are enabled.</li> <li>MySQL Credentials:<ul> <li><code>DB_USER</code>: <code>student</code></li> <li><code>DB_PASS</code>: your project name (e.g., <code>john-doe-pawait-1</code>)</li> <li><code>DB_NAME</code>: <code>cloud_mastery</code></li> <li><code>DB_HOST</code>: your Cloud SQL instance's public IP address.</li> </ul> </li> <li> <p>BigQuery Table ID: You need an existing BigQuery Dataset and table where the pipeline will load the data from our first step.</p> <ul> <li><code>BQ_TABLE_ID</code> = <code>john-higi-pawait-1.john_higi_pawait_1.cloud_mastery</code></li> </ul> </li> </ul>"},{"location":"data-analytics/data-pipeline-create-function/#step-1-prepare-your-code","title":"Step 1: Prepare Your Code","text":"<p>Your function needs a <code>main.py</code> file for the logic and a <code>requirements.txt</code> for dependencies.</p> <p><code>main.py</code> <pre><code>import os\nimport pandas as pd\nimport pymysql\nfrom google.cloud import bigquery\n\ndef denormalize_and_load(request):\n    # Load credentials from environment variables\n    db_user = os.environ['DB_USER']\n    db_pass = os.environ['DB_PASS']\n    db_name = os.environ['DB_NAME']\n    db_host = os.environ['DB_HOST']\n    table_id = os.environ['BQ_TABLE_ID']\n\n    # SQL query to join tables and create a denormalized view\n    query = \"\"\"\n    SELECT\n        o.id AS order_id,\n        o.orderNumber,\n        CONCAT(c.firstName, ' ', c.lastName) AS customer_name,\n        c.email,\n        o.orderDate,\n        od.product_id,\n        p.name AS product_name,\n        p.category,\n        od.quantity,\n        od.unitCost,\n        od.totalCost,\n        o.status,\n        o.paymentMethod\n    FROM orders o\n    JOIN customers c ON o.customer_id = c.id\n    JOIN order_details od ON o.id = od.order_id\n    JOIN products p ON od.product_id = p.id\n    \"\"\"\n\n    # Establish connection and fetch data into a pandas DataFrame\n    connection = pymysql.connect(host=db_host, user=db_user, password=db_pass, database=db_name, port=3306)\n    df = pd.read_sql(query, connection)\n    connection.close()\n\n    # Ensure data types are correct for BigQuery\n    if 'orderDate' in df.columns:\n        df['orderDate'] = pd.to_datetime(df['orderDate'])\n\n    # Load DataFrame into BigQuery\n    client = bigquery.Client()\n    job_config = bigquery.LoadJobConfig(\n        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n        autodetect=True\n    )\n\n    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)\n    job.result()  # Wait for the job to complete\n\n    return f\"Loaded {len(df)} rows to {table_id}.\"\n</code></pre></p> <p><code>requirements.txt</code> <pre><code>functions-framework==3.*\npandas==2.2.2\nSQLAlchemy==2.0.30\nPyMySQL==1.1.0\ngoogle-cloud-bigquery==3.21.0\npyarrow==20.0.0\n</code></pre></p> \u2190 Previous: Setup BigQuery Section 22  - Create Cloud Function Next: Schedule with Cloud Scheduler \u2192"},{"location":"data-analytics/data-pipeline-schedule-job/","title":"3. Schedule the Job with Cloud Scheduler","text":"<p>Now that our ETL function is working, we need to automate it. Cloud Scheduler is a fully-managed cron job service that allows you to schedule tasks, including triggering HTTP endpoints like our Cloud Function.</p>"},{"location":"data-analytics/data-pipeline-schedule-job/#prerequisites","title":"Prerequisites","text":"<ul> <li>A deployed and working Cloud Function.</li> <li>The Cloud Scheduler API enabled in your project.</li> </ul>"},{"location":"data-analytics/data-pipeline-schedule-job/#step-1-create-the-scheduler-job","title":"Step 1: Create the Scheduler Job","text":"<ol> <li> <p>In the Google Cloud Console, search for and navigate to Cloud Scheduler.     </p> </li> <li> <p>Click CREATE JOB.     </p> </li> <li> <p>Define the job:</p> <ul> <li>Name: <code>mysql-to-bq-etl-5min</code></li> <li>Region: <code>us-central1</code> (must match your function's region).</li> <li>Description: <code>Runs the MySQL to BigQuery ETL function every 5 minutes.</code></li> <li>Frequency: Enter the schedule using cron syntax. For every 5 minutes, use: <code>*/5 * * * *</code></li> <li>Timezone: Select your desired timezone.</li> </ul> </li> <li> <p>Configure the execution:</p> <ul> <li>Target type: HTTP.</li> <li>URL: Paste the Cloud Function Trigger URL.</li> <li>HTTP method: GET (or POST).</li> <li>Auth header: Since we allowed unauthenticated invocations, set this to None.</li> </ul> </li> <li> <p>Click CREATE.</p> </li> </ol>"},{"location":"data-analytics/data-pipeline-schedule-job/#step-2-test-and-verify-the-job","title":"Step 2: Test and Verify the Job","text":"<p>You don't have to wait for the schedule to test the job.</p> <ol> <li>On the Cloud Scheduler dashboard, find your new job.</li> <li>Click the three-dot menu (\u22ee) on the right and select Force run.</li> </ol> <p>How to Verify:</p> <ol> <li>Scheduler Log: The Result column for the job should quickly change to \"Success\".</li> <li>Cloud Function Log: Navigate to your function's logs. You should see a new entry showing a successful execution.</li> <li>BigQuery Table: Go to your <code>denormalized_orders</code> table in BigQuery. The \"Last modified\" time should be updated, and the \"Number of rows\" should be greater than zero.</li> </ol>"},{"location":"data-analytics/data-pipeline-schedule-job/#whats-next","title":"What's Next","text":"<p>Your data pipeline is now fully automated! Data will be refreshed in BigQuery every 5 minutes. The final step is to connect Looker Studio to this data and build a compelling visualization.</p> \u2190 Previous: Create Cloud Function Section 23 - Schedule Job Next: Visualize with Looker Studio \u2192"},{"location":"data-analytics/data-pipeline-setup-bigquery/","title":"1. Setup BigQuery: Dataset and Table","text":"<p>The first step in any data pipeline is to prepare the destination. In our case, this is Google BigQuery. A Dataset in BigQuery is a container for your tables (like a database schema), and a Table holds your data in rows and columns.</p>"},{"location":"data-analytics/data-pipeline-setup-bigquery/#step-1-create-a-bigquery-dataset","title":"Step 1: Create a BigQuery Dataset","text":"<ol> <li> <p>Navigate to BigQuery Studio:</p> <ul> <li>In the Google Cloud Console, open the navigation menu (\u2630) and select BigQuery &gt; Studio.</li> </ul> <p></p> </li> <li> <p>Initiate Dataset Creation:</p> <ul> <li>In the Explorer panel, click the three-dot menu (\u22ee) next to your project ID and select Create dataset.</li> </ul> </li> <li> <p>Configure the Dataset:</p> <ul> <li>Dataset ID: <code>cloud_mastery</code></li> <li>Data location: <code>us-central1</code> (or your preferred region).     !!! important         You cannot change the location after creation. Choose a location close to your other services to optimize performance and cost.</li> <li>Click CREATE DATASET. Your new dataset will now appear under your project in the Explorer panel.</li> </ul> </li> </ol>"},{"location":"data-analytics/data-pipeline-setup-bigquery/#step-2-create-a-bigquery-table","title":"Step 2: Create a BigQuery Table","text":"<p>Now, let's create an empty table inside your dataset where the pipeline will load data.</p> <ol> <li> <p>Initiate Table Creation:</p> <ul> <li>In the Explorer panel, click the three-dot menu (\u22ee) next to your <code>cloud_mastery</code> dataset and select Create table.</li> </ul> <p></p> </li> <li> <p>Configure the Table:</p> <ul> <li>Source: Keep the default Empty table.</li> <li>Destination Table name: <code>denormalized_orders</code></li> <li>Schema: Leave this blank for now. We will use BigQuery's <code>autodetect</code> feature when we load the data.</li> </ul> </li> <li> <p>Click CREATE TABLE.</p> </li> </ol> <p>Your new empty table will now appear under its dataset. To view your table name, click on your table in the Explorer, then click on Details. Copy the Table ID for use in the next section.</p> <p></p> <p>BigQuery Destination Ready</p> <p>You have successfully created the dataset and table in BigQuery. This structure is now ready to receive data from our pipeline.</p>"},{"location":"data-analytics/data-pipeline-setup-bigquery/#whats-next","title":"What's Next","text":"<p>With our destination ready, the next step is to build the core of our ETL process: the Cloud Function that will extract data from MySQL and load it into this table.</p> \u2190 Previous: Pipeline Overview Section 21  - Setup BigQuery  Next: Create Cloud Function \u2192"},{"location":"data-analytics/data-pipeline-visualize-looker/","title":"4. Visualize Data with Looker Studio","text":"<p>The final step is to unlock the value in our data by creating an interactive dashboard. Looker Studio (formerly Google Data Studio) is a free tool that turns your data into informative, easy to read, and easy to share dashboards.</p>"},{"location":"data-analytics/data-pipeline-visualize-looker/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Looker Studio account (lookerstudio.google.com).</li> <li>Your BigQuery table containing data.</li> <li>A Looker Studio Dashboard Template to copy.</li> </ul>"},{"location":"data-analytics/data-pipeline-visualize-looker/#step-1-copy-the-dashboard-and-connect-to-your-data","title":"Step 1: Copy the Dashboard and Connect to Your Data","text":"<ol> <li>Open the provided Looker Studio report template.</li> <li>In the top-right corner, click the three-dot menu (\u22ee) and select Make a copy.     </li> <li>In the \"Copy this report\" window, under the New Data Source column, click the current data source to change it.     </li> <li>In the panel that opens, click CREATE DATA SOURCE at the bottom.</li> <li>Select the BigQuery connector.     </li> <li>Navigate through the hierarchy: Your Project &gt; <code>cloud_mastery</code> Dataset &gt; <code>denormalized_orders</code> Table.</li> <li>Click CONNECT in the top-right.     </li> <li>Looker will show you the table schema. Click ADD TO REPORT.     </li> <li>You'll be back at the \"Copy this report\" window. Confirm your new data source is selected and click COPY REPORT.</li> </ol> <p>Your dashboard copy will now load, powered by your own BigQuery data. </p>"},{"location":"data-analytics/data-pipeline-visualize-looker/#step-2-add-a-calculated-field","title":"Step 2: Add a Calculated Field","text":"<p>Let's create a new field directly in Looker Studio without altering our BigQuery table. We'll group detailed order statuses into simpler categories. </p> <ol> <li>In your new report, go to the menu Resource &gt; Manage added data sources.</li> <li>Find your <code>denormalized_orders</code> data source and click EDIT.</li> <li>In the top-left of the data source editor, click + ADD A FIELD.</li> <li>Configure the new field:<ul> <li>Field Name: <code>Order Status Group</code></li> <li>Formula: <pre><code>-- This formula groups detailed statuses into broader categories.\nCASE\n    WHEN status = 'Delivered' THEN 'Completed'\n    WHEN status = 'Shipped' THEN 'Completed'\n    WHEN status = 'Processing' THEN 'In Progress'\n    WHEN status = 'Pending' THEN 'In Progress'\n    ELSE 'Other'\nEND\n</code></pre> </li> </ul> </li> <li>Click SAVE. Then click DONE in the top-right to return to the report.</li> </ol>"},{"location":"data-analytics/data-pipeline-visualize-looker/#step-3-use-the-calculated-field-in-a-new-chart","title":"Step 3: Use the Calculated Field in a New Chart","text":"<ol> <li>In the top menu, click Add a chart and select Pie chart.     </li> <li>Click on the report canvas to place the chart.</li> <li>With the new chart selected, configure its Setup panel on the right:<ul> <li>Dimension: Drag your new <code>Order Status Group</code> field here.</li> <li>Metric: Drag the <code>order_id</code> field here and set its aggregation to Count Distinct (CTD). </li> </ul> </li> <li>Use the Style tab in the right-hand panel to customize its appearance.     </li> </ol> <p>Lab Complete!</p> <p>You have successfully built an end-to-end data pipeline, from raw data in MySQL to an automated data warehouse in BigQuery, and finally to an interactive dashboard in Looker Studio.</p> \u2190 Previous: Schedule Job Section 24 - Visualize Data Next: Virtual Agents Lab \u2192"},{"location":"devops/accessing-the-application/","title":"7. Deploy the Frontend and Access Your Application","text":"<p>This is the final step where everything comes together. We will trigger the deployment for our frontend application the same way we did for the backend. Once the pipeline succeeds, we will access the public URL to see our live, fully-functional dashboard.</p>"},{"location":"devops/accessing-the-application/#trigger-the-frontend-deployment","title":"Trigger the Frontend Deployment","text":"<p>Let's make a final code push to trigger the frontend build pipeline.</p> <ol> <li> <p>In the Cloud Shell Editor's EXPLORER pane, you will see the <code>cloud-mastery-frontend</code> directory.     </p> </li> <li> <p>Expand the <code>cloud-mastery-frontend</code> folder and click on its <code>README.md</code> file to open it.     </p> </li> <li> <p>Make a small change to this file, just as you did for the backend. When you're done, return to the Cloud Shell terminal.</p> </li> <li> <p>Now, run the following commands to push your change to GitHub.</p> <p>Just Like Before</p> <p>This process is identical to what we did for the backend. We navigate to the correct folder, add our changes, commit them, and push to the <code>master</code> branch to trigger our build.</p> <p><pre><code># IMPORTANT: Navigate to the frontend directory first!\ncd ~/cloud-mastery-frontend\n\n# Add, commit, and push the changes\ngit add .\ngit commit -m \"Deploy frontend application\"\ngit push origin master\n</code></pre> Your push will now trigger the <code>cloud-mastery-frontend-deploy</code> pipeline. </p> </li> </ol>"},{"location":"devops/accessing-the-application/#monitor-the-build-and-verify-deployment","title":"Monitor the Build and Verify Deployment","text":"<ol> <li> <p>Navigate back to the Cloud Build History page in the Google Cloud Console. You will see the new build for the frontend running.</p> <p>Faster Build Time</p> <p>You'll notice this build completes much faster than the backend build, as it doesn't involve database migrations.</p> <p></p> </li> <li> <p>Once the build shows a green Successful status, your frontend is officially deployed!     </p> </li> </ol>"},{"location":"devops/accessing-the-application/#access-your-live-application","title":"Access Your Live Application","text":"<p>This is the moment of truth! Let's view the live application.</p> <ol> <li> <p>In the Google Cloud Console, navigate to the Cloud Run page.</p> </li> <li> <p>Click on the <code>cloud-mastery-frontend</code> service to open its details.</p> </li> <li> <p>At the top of the service details page, you'll find the application's public URL. Click on it to open your dashboard in a new tab.     </p> </li> <li> <p>Behold your fully deployed application dashboard!     </p> </li> </ol> <p>Congratulations! Project Complete!</p> <p>You have successfully built and deployed a full-stack, database-driven application on Google Cloud using modern, automated practices.</p> <p>Throughout this lab, you have accomplished a great deal:</p> <ul> <li>Prepared a Google Cloud environment and mastered the Cloud Shell.</li> <li>Provisioned and populated a managed Cloud SQL database.</li> <li>Forked repositories and configured GitHub with SSH for secure access.</li> <li>Built two separate CI/CD pipelines with Cloud Build for automated deployments.</li> <li>Passed secrets securely to the build process using substitution variables.</li> <li>Deployed both backend and frontend containerized services to Cloud Run.</li> </ul> \u2190 Previous: Setup Frontend Pipeline Section 19 -  Access the application  Next: Analytics Lab \u2192"},{"location":"devops/devops-lab/","title":"Automate Securely: DevOps &amp; CI/CD on GCP Lab","text":""},{"location":"devops/devops-lab/#overview","title":"Overview","text":"<p>Learn DevOps on GCP for automated app updates (CI/CD) and security by building a pipeline and covering GCP security basics.</p>"},{"location":"devops/devops-lab/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>Using Cloud Shell to access git repos and edit code</li> <li>Setup SSH keys and add public key to Github</li> <li>Configuring databases via Cloud SQL</li> <li>Setting up CI/CD pipelines with Google Cloud Build</li> <li>Pushing docker images to Google Artifact Registry</li> <li>Securely injecting application secrets</li> <li>Deploying containerized applications to serverless environments</li> <li>Embracing Devops Best Practices</li> <li>Securing your application within GCP</li> </ul>"},{"location":"devops/devops-lab/#prerequisites","title":"Prerequisites","text":"<ul> <li>Active Google account</li> <li>Basic understanding of cloud computing concepts</li> <li>Basic Linux knowledge</li> <li>Basic Git knowledge</li> </ul>"},{"location":"devops/devops-lab/#step-1-activate-cloud-shell","title":"Step 1: Activate Cloud Shell","text":"<p>Cloud Shell provides a command-line environment directly in your browser, with the <code>gcloud</code> SDK and other utilities pre-installed.</p> <ol> <li> <p>Open the Cloud Shell by clicking on the Activate Cloud Shell icon in the top-right corner of the console, as highlighted below.</p> <p></p> </li> <li> <p>A new frame will open at the bottom of your window. Click Continue to proceed.</p> <p></p> </li> <li> <p>You will be prompted to grant Cloud Shell permission to use your account credentials to make API calls to Google Cloud. Click Authorize.</p> <p></p> </li> <li> <p>On some occasions, you may be asked to re-enter your password to continue.</p> <p></p> </li> </ol>"},{"location":"devops/devops-lab/#step-2-open-cloud-shell-in-a-new-tab","title":"Step 2: Open Cloud Shell in a New Tab","text":"<p>For a better experience, we will open the Cloud Shell in its own dedicated browser tab.</p> <ol> <li> <p>When the provisioning is complete, your Cloud Shell terminal is ready.</p> <p></p> </li> <li> <p>Click the Open in new window button to launch the Cloud Shell in a new tab.</p> <p></p> </li> <li> <p>The Cloud Shell will now be open in a new, full-screen browser tab. You may be asked to Authorize again.</p> <p></p> </li> </ol>"},{"location":"devops/devops-lab/#next-steps","title":"Next Steps","text":"<p>You are now ready for the next section! Your Google Cloud environment is prepared, and your Cloud Shell is active. In the next section, we will set up the Cloud SQL database.</p> \u2190 Previous: GCP Migration Lab Section 12 -  DevOps Lab  Next: Database Setup \u2192"},{"location":"devops/setup-backend-pipeline/","title":"Automating Backend Deployments with Cloud Build","text":"<p>In this section, we'll harness the power of Google Cloud Build to create a Continuous Integration (CI) pipeline. This is a crucial step in modern software development.</p> <p>Once configured, this pipeline will automatically build and deploy our backend application to Cloud Run every time we push new code to our GitHub repository. No more manual deployments!</p>"},{"location":"devops/setup-backend-pipeline/#step-1-create-a-cloud-build-trigger","title":"Step 1: Create a Cloud Build Trigger","text":"<p>Let's start by creating the trigger that will listen for changes in our repository.</p> <ol> <li> <p>In the Google Cloud Console, use the top search bar to find <code>Cloud Build</code>, then select Triggers from the results.     </p> </li> <li> <p>On the Triggers page, click Create trigger.     </p> </li> </ol>"},{"location":"devops/setup-backend-pipeline/#step-2-configure-the-trigger","title":"Step 2: Configure the Trigger","text":"<p>Now, we'll configure the trigger to connect to our specific GitHub repository and run our deployment instructions.</p> <ol> <li>Name: Give your trigger a descriptive name, like <code>cloud-mastery-backend-deploy</code>.</li> <li>Region: Select <code>us-central1 (Iowa)</code>.</li> <li>Event: Keep the default setting, <code>Push to a branch</code>.</li> <li>Source - Connect your GitHub repository:<ol> <li>Next to Repository, click Connect new repository.     </li> <li>Select GitHub (Cloud Build GitHub App) and click Continue.</li> <li>Authorize the Google Cloud Build app to access your GitHub account.     </li> <li>On the GitHub installation page, select All repositories and click Install. This grants Cloud Build permission to see your repositories.     </li> <li>Back in the Cloud Build console, select your forked <code>your-github-username/cloud-mastery-backend</code> repository from the dropdown.</li> <li>Check the consent box and click Connect.     </li> </ol> </li> <li> <p>Source - Branch: In the Branch field, enter <code>^master$</code>.</p> <p>What does <code>^master$</code> mean?</p> <p>This is a regular expression that ensures the trigger only runs for pushes made directly to the <code>master</code> branch, ignoring other branches.</p> </li> <li> <p>Configuration - Build Configuration:</p> <ul> <li>Type: Select <code>Cloud Build configuration file (yaml or json)</code>.</li> <li>Location: Leave the default <code>Repository</code> setting. The path <code>/cloudbuild.yaml</code> points to the deployment instructions file already in your repository.</li> </ul> </li> <li> <p>Advanced - Substitution Variables:</p> <ul> <li>We need to securely provide our database password to the build process. We'll do this using a substitution variable.</li> <li>Click Add variable.<ul> <li>Variable: <code>_MYSQL_PRISMA_URL</code></li> <li>Value: This is the full connection string for your database. To build it:<ol> <li>Open a new browser tab and navigate to the Cloud SQL instances page.</li> <li>Copy the Public IP address of your <code>cloud-mastery-sql</code> instance.     </li> <li>Construct the connection string using the format below.</li> </ol> </li> </ul> </li> </ul> <p>Action Required: Replace Placeholders</p> <p>You must replace the following placeholders in the string below: *   <code>YOUR_PROJECT_ID</code>: Find this on your Google Cloud Console dashboard. *   <code>YOUR_INSTANCE_PUBLIC_IP</code>: The public IP address you just copied.</p> <p><pre><code>mysql://student:YOUR_PROJECT_ID@YOUR_INSTANCE_PUBLIC_IP:3306/cloud_mastery?sslmode=require\n</code></pre> *   Paste your completed and correct string into the Value field.     </p> </li> <li> <p>Advanced - Service Account:</p> <ul> <li>Scroll down to the Service Account dropdown.</li> <li>Select the service account that starts with <code>cloud-mastery-</code>. This special service account has the necessary permissions (like deploying to Cloud Run) to execute our pipeline.     </li> </ul> </li> <li> <p>Click the Create button at the bottom of the page to save your trigger.</p> </li> </ol>"},{"location":"devops/setup-backend-pipeline/#step-3-trigger-your-first-automated-deployment","title":"Step 3: Trigger Your First Automated Deployment","text":"<p>To trigger our pipeline, we need to push a new commit to our GitHub repository. We'll make a small, cosmetic change to do this.</p> <ol> <li>In your Cloud Shell, click the Open Editor (pencil) icon in the top-right.     </li> <li>The editor will open. In the left-hand navigation pane, click the Explorer (two pages) icon, then click the Open Folder button.      </li> <li>A dialog will ask for confirmation. Click OK to open the <code>cloud-mastery-backend</code> project.     </li> <li>In the explorer, click on the <code>README.md</code> file to open it for editing.     </li> <li>Make any small change to the file, like adding a new line or fixing a typo. The content of the change doesn't matter.</li> <li>Now, return to the main Cloud Shell terminal (the black window at the bottom).</li> <li> <p>Execute the following commands one by one to set up your Git identity, commit the file, and push it to GitHub.</p> <p>Use Your Training Git Identity</p> <p>Replace the placeholder email and name with the details provided for this training.</p> <p><pre><code># Ensure you are in the correct directory\ncd ~/cloud-mastery-backend\n\n# Configure your Git identity for this commit (one-time setup)\ngit config --global user.email \"firstname.lastname1@train.pawait.co.ke\"\ngit config --global user.name \"First Name Last Name\"\n\n# Stage, commit, and push your change to trigger the build\ngit add .\ngit commit -m \"Trigger initial Cloud Build deployment\"\ngit push origin master\n</code></pre> Your push to the <code>master</code> branch is the event that our Cloud Build trigger is waiting for! </p> </li> </ol>"},{"location":"devops/setup-backend-pipeline/#step-4-monitor-the-build-and-verify-deployment","title":"Step 4: Monitor the Build and Verify Deployment","text":"<ol> <li> <p>Return to the Google Cloud Console and navigate to the Cloud Build History page. You'll see your build kick off automatically, listed with a \"Running\" status. Click on it to view the live logs.      </p> </li> <li> <p>Patience is a virtue! The build process will take about 7 minutes to complete as it builds the container image and deploys it to Cloud Run. Once done, you'll see a green \"Successful\" status.     </p> </li> <li> <p>Let's see the result! Navigate to Cloud Run in the console (or use this direct link: console.cloud.google.com/run). You'll now see your <code>cloud-mastery-backend</code> service listed with a green checkmark, indicating it's deployed and healthy.     </p> </li> </ol> <p>Congratulations! Backend Automation Complete!</p> <p>You have successfully configured a professional Continuous Integration / Continuous Deployment (CI/CD) pipeline. From now on, every time you <code>git push</code> a change to the <code>master</code> branch, Cloud Build will automatically handle the deployment for you.</p>"},{"location":"devops/setup-backend-pipeline/#next-steps","title":"Next Steps","text":"<p>Great job! Next, we will apply these same principles to our frontend application.</p> \u2190 Previous: Setup Backend Repository Section 16 -  Setup Backend Pipeline  Next: Setup Frontend Repo \u2192"},{"location":"devops/setup-backend-repository/","title":"Prepare Backend Repo","text":""},{"location":"devops/setup-backend-repository/#step-1-fork-the-backend-repository","title":"Step 1: Fork the Backend Repository","text":"<p>You need your own copy of the application repository to make changes. This is done by \"forking\" it.</p> <ol> <li> <p>Access the Cloud Mastery backend repository here:     https://github.com/Pawa-IT-Solutions/cloud-mastery-backend</p> </li> <li> <p>Click the Fork button in the top-right corner.</p> <p></p> </li> <li> <p>On the \"Create a new fork\" page, you can leave the details as they are and click Create fork.</p> <p></p> </li> <li> <p>You will be redirected to your own forked copy of the repository. It is now ready!</p> <p></p> </li> </ol>"},{"location":"devops/setup-backend-repository/#step-2-clone-the-repository-to-cloud-shell","title":"Step 2: Clone the Repository to Cloud Shell","text":"<p>Now you can clone your forked repository.</p> <ol> <li> <p>In GitHub, navigate to your forked <code>cloud-mastery-backend</code> repository. Click the green &lt;&gt; Code button, select the SSH tab, and copy the SSH URL.</p> <p></p> </li> <li> <p>Go back to your Cloud Shell terminal and run the <code>git clone</code> command, pasting the URL you just copied.     Replace the URL with your own forked repository SSH URL</p> <pre><code>git clone git@github.com:austinkaruru1/cloud-mastery-backend.git\n</code></pre> </li> <li> <p>When prompted <code>Are you sure you want to continue connecting (yes/no/[fingerprint])?</code>, type <code>yes</code> and press <code>Enter</code>.</p> <p></p> </li> <li> <p>Navigate into the newly created directory and list its contents.</p> <p><pre><code>cd cloud-mastery-backend &amp;&amp; ls -l\n</code></pre> </p> </li> </ol>"},{"location":"devops/setup-backend-repository/#next-steps","title":"Next Steps","text":"<p>Great job! Your GitHub account is configured, and you have successfully cloned the backend application code into your Cloud Shell. Next, we will set up the continuous integration pipeline using Cloud Build.</p> \u2190 Previous: Setup Github Section 15 -  Prepare Backend Repo  Next: Setup Backend Pipeline \u2192"},{"location":"devops/setup-cloud-sql/","title":"Setup Database in Cloud SQL","text":"<p>Next, we need to set up the database to ensure all the necessary data is populated for our application. We will do this by importing a pre-configured SQL file into our Cloud SQL instance.</p>"},{"location":"devops/setup-cloud-sql/#step-1-navigate-to-cloud-sql","title":"Step 1: Navigate to Cloud SQL","text":"<ol> <li> <p>Navigate to the Cloud SQL instances page by searching for \"Cloud SQL\" in the top search bar or by using this direct link:     https://console.cloud.google.com/sql/instances</p> </li> <li> <p>You should see that a Cloud SQL instance has already been provisioned for you.</p> <p></p> </li> </ol>"},{"location":"devops/setup-cloud-sql/#step-2-import-the-database","title":"Step 2: Import the Database","text":"<ol> <li> <p>Click on the Instance ID to open the instance details page.</p> </li> <li> <p>At the top of the page, click the IMPORT button.</p> <p></p> </li> <li> <p>On the \"Import data\" screen, under the Select source file section, click BROWSE.</p> <p></p> </li> <li> <p>A popup window will appear showing your Cloud Storage buckets. Double-click on the bucket name that ends with <code>-cloud-mastery</code>.</p> <p></p> </li> <li> <p>Select the <code>cloud_mastery.sql</code> file and click the SELECT button at the bottom.</p> <p></p> </li> <li> <p>Finally, under the Destination section, expand the Database dropdown and select <code>cloud_mastery</code>.</p> <p></p> </li> <li> <p>Click the IMPORT button to start the process. The import will begin, and you will be returned to the instance details page.</p> </li> </ol>"},{"location":"devops/setup-cloud-sql/#next-steps","title":"Next Steps","text":"<p>Database setup is complete! You can now proceed to the next step, where we will prepare our GitHub environment.</p> \u2190 Previous: DevOps Lab Section 13 -  Database Setup  Next: Setup Github \u2192"},{"location":"devops/setup-frontend-pipeline/","title":"Setup Cloud Build for the Frontend","text":"<p>We will now create the final CI/CD pipeline for our frontend application. This process is very similar to the backend setup but requires different environment variables for the application to connect to the backend API.</p>"},{"location":"devops/setup-frontend-pipeline/#step-1-create-the-frontend-trigger","title":"Step 1: Create the Frontend Trigger","text":"<ol> <li>Navigate back to Cloud Build &gt; Triggers in the Google Cloud console.</li> <li>Click Create trigger.</li> </ol>"},{"location":"devops/setup-frontend-pipeline/#step-2-configure-the-trigger","title":"Step 2: Configure the Trigger","text":"<ol> <li>Name: <code>cloud-mastery-frontend-deploy</code></li> <li>Region: <code>us-central1 (Iowa)</code></li> <li>Source:<ul> <li>Repository: Select your forked <code>austinkaruru1/cloud-mastery-frontend</code> repository from the dropdown. It should already be available since we connected your GitHub account.      </li> </ul> </li> <li>Branch: Enter <code>^master$</code></li> <li>Configuration:<ul> <li>Type: <code>Cloud Build configuration file (yaml or json)</code></li> <li>Location: <code>/cloudbuild.yaml</code> (default)</li> </ul> </li> <li> <p>Advanced: Substitution Variables:</p> <ul> <li>This time, we need to add two variables. Click Add variable twice.</li> </ul> <p>Important: Backend URL Needed</p> <p>One of the variables requires the URL of the <code>cloud-mastery-backend</code> service you deployed in a previous step.</p> <ul> <li>Variable 1:<ul> <li>Variable: <code>_NEXT_PUBLIC_APP_NAME</code></li> <li>Value: <code>CLOUD MASTERY TRAINING</code></li> </ul> </li> <li>Variable 2:<ul> <li>Variable: <code>_NEXT_PUBLIC_API_URL</code></li> <li>Value: To get this value, open a new tab and go to the Cloud Run console. Click on your <code>cloud-mastery-backend</code> service and copy its URL.     </li> <li>Paste the URL into the value field and append <code>/api/v1</code> to the end. It should look like this:     <pre><code>https://cloud-mastery-backend-xxxxxxxxxx.us-central1.run.app/api/v1\n</code></pre> </li> </ul> </li> </ul> </li> <li> <p>Service Account: Select the <code>terraform-</code> service account, just as you did for the backend.</p> </li> <li> <p>Click Create. Your Cloud Build Triggers list should now show both the backend and frontend triggers.</p> <p></p> </li> </ol>"},{"location":"devops/setup-frontend-pipeline/#next-steps","title":"Next Steps","text":"<p>Frontend pipeline is configured! In the final step, we will run this trigger and access our fully deployed application.</p> \u2190 Previous: Setup Frontend Repo Section 18 -  Setup Frontend Pipeline  Next: Access the Application \u2192"},{"location":"devops/setup-frontend-repository/","title":"Setup the Frontend Repository","text":"<p>Now we will repeat the process for the <code>cloud-mastery-frontend</code> application. We will fork the repository to our GitHub account and then clone it into our Cloud Shell environment.</p>"},{"location":"devops/setup-frontend-repository/#step-1-fork-the-frontend-repository","title":"Step 1: Fork the Frontend Repository","text":"<ol> <li> <p>Access the Cloud Mastery frontend repository here:     https://github.com/Pawa-IT-Solutions/cloud-mastery-frontend</p> </li> <li> <p>Just like before, click the Fork button in the top-right corner.</p> <p></p> </li> <li> <p>On the \"Create a new fork\" page, you can accept the default settings and click Create fork.</p> <p></p> </li> <li> <p>You will be redirected to your personal forked copy of the <code>cloud-mastery-frontend</code> repository.</p> <p></p> </li> </ol>"},{"location":"devops/setup-frontend-repository/#step-2-clone-the-frontend-repository-to-cloud-shell","title":"Step 2: Clone the Frontend Repository to Cloud Shell","text":"<p>We will clone the frontend into a separate directory within our Cloud Shell home directory.</p> <ol> <li> <p>In GitHub, navigate to your forked <code>cloud-mastery-frontend</code> repository. Click the green &lt;&gt; Code button, select the SSH tab, and copy the SSH URL.</p> <p></p> </li> <li> <p>Navigate back to your Google Cloud Shell tab. Your current directory should be <code>~/cloud-mastery-backend</code>. First, go back to your home directory.</p> <pre><code>cd ~\n</code></pre> <p>Working with Multiple Terminals     Open a new Cloud Shell terminal tab by clicking the <code>+</code> icon. This is a great way to manage separate tasks. For this guide, we will perform the clone from the home directory in the same terminal.</p> <p> </p> </li> <li> <p>Run the <code>git clone</code> command, pasting the frontend repository's SSH URL you just copied.     Replace the URL with your own forked repository SSH URL</p> <pre><code>git clone git@github.com:austinkaruru1/cloud-mastery-frontend.git\n</code></pre> <p></p> </li> <li> <p>List the contents of your home directory. You should now see folders for both the backend and frontend repositories.</p> <p><pre><code>ls -l\n</code></pre> </p> </li> <li> <p>Navigate into the new frontend directory to confirm the files are there.     <pre><code>cd cloud-mastery-frontend &amp;&amp; ls -l\n</code></pre></p> <p></p> </li> </ol>"},{"location":"devops/setup-frontend-repository/#next-steps","title":"Next Steps","text":"<p>Frontend repository is ready! You now have the source code for both the backend and frontend applications in your Cloud Shell. Next, we will create the Cloud Build trigger to automate the deployment of the frontend.</p> \u2190 Previous: Setup Backend Pipeline Section 17  -  Setup Frontend Repo  Next: Setup Frontend Pipeline \u2192"},{"location":"devops/setup-github/","title":"Prepare GitHub Environment","text":"<p>For this lab, we will work with two application repositories: <code>cloud-mastery-backend</code> and <code>cloud-mastery-frontend</code>. First, we need to set up your GitHub account and configure it to work with your Google Cloud Shell environment.</p> <p>Do you have a GitHub Account?</p> <p>If you already have a GitHub account, you can skip directly to the next section: Setup Backend Repository</p>"},{"location":"devops/setup-github/#step-1-create-or-login-to-an-existing-github-account","title":"Step 1: Create or Login to an existing GitHub Account","text":"<ol> <li> <p>Navigate to the GitHub signup page: github.com/signup.</p> </li> <li> <p>Fill in your details (email, password, username) to create your account.</p> <p></p> </li> <li> <p>Complete the \"Verify your account\" puzzle to prove you're human.</p> <p></p> </li> <li> <p>GitHub will send a verification code to your email address. Enter this code to confirm your email.</p> <p></p> </li> <li> <p>Once verified, proceed to sign in. Your new GitHub account is now ready!</p> <p></p> </li> </ol>"},{"location":"devops/setup-github/#step-2-setup-ssh-key-from-cloud-shell","title":"Step 2: Setup SSH Key from Cloud Shell","text":"<p>To securely clone the repository to your Cloud Shell, you need to add your Cloud Shell's SSH key to your GitHub account.</p> <p>In the context of GitHub, public and private keys are used for secure authentication, primarily through SSH. The private key is kept secret on your local machine, while the corresponding public key is shared with GitHub. This allows GitHub to verify your identity when you perform actions like pushing to or pulling from a repository. </p> <ol> <li> <p>Navigate back to your Google Cloud Shell tab.</p> </li> <li> <p>Run the <code>ssh-keygen</code> command to generate a new SSH key. Press <code>Enter</code> three times to accept the default file location and create a key without a passphrase.</p> <p><pre><code>ssh-keygen\n</code></pre> </p> </li> <li> <p>Verify that the public key file (<code>id_ed25519.pub</code>) was created.</p> <p><pre><code>ls -l .ssh/\n</code></pre> </p> </li> <li> <p>Display the public key and copy its entire content to your clipboard.</p> <p><pre><code>cat .ssh/id_ed25519.pub\n</code></pre> </p> </li> <li> <p>Head back to your GitHub tab. Click on your profile icon in the top-right corner and select Settings.</p> <p> </p> </li> <li> <p>In the left navigation menu, click on SSH and GPG keys.</p> <p></p> </li> <li> <p>Click New SSH key. Give it a descriptive Title (e.g., \"Google Cloud Shell\") and paste the copied key into the Key field. Click Add SSH key.</p> <p></p> </li> </ol>"},{"location":"devops/setup-github/#next-steps","title":"Next Steps","text":"<p>Github setup is complete! You can now proceed to the next step, where we will fork and clone backend repo.</p> \u2190 Previous: Database Setup Section 14 -  Github Setup  Next: Setup Backend Repo \u2192"},{"location":"migration/migration-access-vm/","title":"Access and Configure the Migrated VM","text":"<p>Your migrated virtual machine is running on Google Cloud, but the services for your application are still stopped from the imaging process. In this phase, you'll log in, restart everything, and verify that the application is fully operational.</p>"},{"location":"migration/migration-access-vm/#step-1-access-the-vm-via-ssh","title":"Step 1: Access the VM via SSH","text":"<p>You will connect to your new GCP VM using the same credentials (username and password) that you used for the original on-prem server.</p> <ol> <li>In the GCP Console, go to Compute Engine &gt; VM instances and find the External IP address of your new VM (<code>pawait-vm-gcp-final</code>).</li> <li>Open your preferred SSH client (like Termius, PuTTY, or a standard terminal).</li> <li>Connect to the VM using the following details:<ul> <li>Host/Address: The External IP address from the GCP Console (e.g., <code>34.133.9.165</code>).</li> <li>Username: <code>root</code></li> <li>Password: <code>cloud-mastery</code></li> </ul> </li> </ol> <p>Use Original Credentials</p> <p>It is critical to remember that you are logging in with the credentials from the original machine, not with any new GCP-generated keys. The cloning process copied the entire user and password database.</p> <p></p>"},{"location":"migration/migration-access-vm/#step-2-restart-all-services","title":"Step 2: Restart All Services","text":"<p>Once logged in, you need to restart all the services that you stopped before creating the disk image.</p> <p>Run the following commands one by one to bring all the application and system services back online.</p> <p>Restart lower-priority logging and utility services</p> <pre><code>sudo systemctl start systemd-timesyncd.service\nsudo systemctl start qemu-guest-agent.service\nsudo systemctl start rsyslog.service\n</code></pre> <p>Restart high-priority application and job services <pre><code>sudo systemctl start unattended-upgrades.service\nsudo systemctl start atd.service\nsudo systemctl start cron.service\nsudo systemctl start containerd.service\nsudo systemctl start docker.service\n</code></pre> Confirm that the services are running <pre><code>echo \"--- All services have been started. The migrated VM is now fully operational. ---\"\n</code></pre></p> \u2190 Previous: Create GCP VM Section 10 - Access &amp; Configure VM Next: Access via IP \u2192"},{"location":"migration/migration-create-image/","title":"Creating the Virtual Disk Image (VMDK)","text":"<p>This phase walks you through the process of creating the clone of the virtual machine in a VMDK (Virtual Machine Disk) format.</p>"},{"location":"migration/migration-create-image/#step-1-create-a-raw-image-of-the-os-disk","title":"Step 1: Create a Raw Image of the OS Disk","text":"<p>This command makes a perfect, bit-for-bit photocopy of your main OS disk and saves it to the helper disk.</p> <p>Important Note</p> <p>This step will take a significant amount of time due to the size of the disk. Please let it run to completion.</p> <pre><code>sudo dd if=/dev/sda of=/mnt/imagedisk/ubuntu-vm.img bs=4M status=progress\n</code></pre> <p>Command Explanation:</p> <ul> <li><code>if=/dev/sda</code>: The input file is your main OS disk</li> <li><code>of=/mnt/imagedisk/ubuntu-vm.img</code>: The output file is a new file on your helper disk</li> </ul> <p>This command will read your entire 50 GB OS disk (<code>/dev/sda</code>) and write a raw image file to your new 120 GB disk (<code>/mnt/imagedisk</code>).</p> <p></p>"},{"location":"migration/migration-create-image/#step-2-verify-the-raw-image","title":"Step 2: Verify the Raw Image","text":"<p>After the <code>dd</code> command finishes, it is critical that we verify the file is the correct size:</p> <pre><code>ls -lh /mnt/imagedisk/ubuntu-vm.img\n</code></pre>"},{"location":"migration/migration-create-image/#step-3-convert-the-raw-image-to-the-optimized-vmdk-format","title":"Step 3: Convert the Raw Image to the Optimized VMDK Format","text":"<p>The raw image is huge. This step converts it into a smaller, more efficient VMDK format that Google Cloud prefers.</p> <p>Pro Tip</p> <p>The <code>-O</code> option below is an uppercase letter O (for Output), not the number zero (<code>-0</code>). This is a very common and hard-to-see typo.</p> <pre><code>qemu-img convert -f raw -O vmdk /mnt/imagedisk/ubuntu-vm.img /mnt/imagedisk/ubuntu-vm.vmdk\n</code></pre>"},{"location":"migration/migration-create-image/#step-4-verify-that-the-vmdk-and-image-files-are-created","title":"Step 4: Verify that the VMDK and Image Files are Created","text":"<p>Check that the conversion was successful:</p> <p><pre><code>ls -lh /mnt/imagedisk/\n</code></pre> </p> <p>You should see the large <code>.img</code> file and a much smaller <code>.vmdk</code> file. The <code>.vmdk</code> is what you will upload to a Google Cloud Storage Bucket.</p> <p>Congratulations!</p> <p>You have successfully completed the most difficult part of the process! The VMDK file is now ready for upload to Google Cloud.</p> \u2190 Previous: Prepare On-Prem VM Section 3 -  Create VM Disk Image Next: Create GCS Bucket \u2192"},{"location":"migration/migration-create-vm/","title":"Create a Google Cloud VM Instance","text":"<p>With your custom image ready and your VPC configured, it's time to create the new virtual machine that will run your migrated application.</p>"},{"location":"migration/migration-create-vm/#step-1-create-a-new-instance","title":"Step 1: Create a New Instance","text":"<ol> <li>In the Google Cloud Console, navigate to Compute Engine &gt; VM instances.</li> <li>Click the [+] CREATE INSTANCE button at the top of the page.</li> </ol>"},{"location":"migration/migration-create-vm/#step-2-configure-the-vm-instance","title":"Step 2: Configure the VM Instance","text":"<p>Fill out the configuration details for your new VM.</p> <ul> <li> <p>Name: <code>pawait-vm-gcp-final</code> (or another descriptive name).</p> </li> <li> <p>Region and Zone:</p> <ul> <li>Region: <code>us-central1</code> (Choose the same region as your VPC).</li> <li>Zone: Select any zone within that region, for example, <code>us-central1-c</code>.</li> </ul> </li> <li> <p>Machine configuration:</p> <ul> <li>Series: <code>E2</code></li> <li>Machine type: <code>e2-medium</code> (2 vCPU, 4 GB memory).</li> </ul> </li> </ul> <p></p> <p>Tip</p> <p>Try to select a machine type that closely matches the CPU and RAM of your original on-prem VM. You can always resize it later.</p>"},{"location":"migration/migration-create-vm/#configure-the-boot-disk","title":"Configure the Boot Disk","text":"<p>This is the most critical part of the configuration. You will tell GCP to use the custom image you created in the previous phase.</p> <ol> <li>In the Boot disk section, click the CHANGE button.</li> <li>A new panel will open. Click on the CUSTOM IMAGES tab.</li> <li>From the Image dropdown list, select the image you just created: <code>cloud-mastery-image</code>.</li> <li>The Boot disk type and Size (GB) will default based on the image. You can increase the size if needed.</li> <li>Click Select.</li> </ol> <p> </p>"},{"location":"migration/migration-create-vm/#configure-networking-and-firewall","title":"Configure Networking and Firewall","text":"<ol> <li>Expand the Advanced options section.</li> <li>Click on Networking.</li> <li>Under Network interfaces, ensure the following is selected:<ul> <li>Network: <code>my-cloud-mastery-vpc</code></li> <li>Subnetwork: <code>application-subnet</code></li> </ul> </li> <li>Under Firewall, check the boxes for:<ul> <li><code>Allow HTTP traffic</code></li> <li><code>Allow HTTPS traffic</code> </li> </ul> </li> <li>Review all the settings and click the CREATE button at the bottom of the page.</li> </ol> <p>VM Instance Created</p> <p>Your new virtual machine will now be provisioned and started. Once it's running, you will see a green checkmark next to its name in the VM instances list, and it will have both an Internal and an External IP address.</p> <p></p>"},{"location":"migration/migration-create-vm/#whats-next","title":"What's Next","text":"<p>Your migrated VM is now running in the cloud! The next step is to access it, restart the necessary services, and verify that your application is working correctly.</p> \u2190 Previous: Import VMDK as Image Section 9 - Create GCP VM Instance Next: Access &amp; Configure VM \u2192"},{"location":"migration/migration-create-vpc/","title":"Create a Google Cloud VPC","text":"<p>It's a best practice to create a dedicated network for your new application within your Google Cloud Project. This provides better security, isolation, and control over your network resources.</p>"},{"location":"migration/migration-create-vpc/#step-1-create-the-custom-vpc-network","title":"Step 1: Create the Custom VPC Network","text":"<ol> <li>Navigate to the VPC Networks page in the Google Cloud Console within your project: https://console.cloud.google.com/vpc/networks</li> <li>Click Create VPC network </li> </ol>"},{"location":"migration/migration-create-vpc/#configure-vpc-settings","title":"Configure VPC Settings","text":"<p>Basic VPC Configuration:</p> <ul> <li>Name: <code>my-cloud-mastery-vpc</code></li> <li>Description: VPC for migrated application</li> <li>Subnet creation mode: Select Custom </li> </ul>"},{"location":"migration/migration-create-vpc/#configure-subnet","title":"Configure Subnet","text":"<p>Under New subnet, provide these details:</p> <ul> <li>Name: <code>application-subnet</code></li> <li>Region: Choose a region where you plan to deploy resources, such as <code>us-central1</code></li> <li>IP address range: Define a private IP range in CIDR notation  <ul> <li>Recommended: <code>10.10.1.0/24</code> (This gives you 256 addresses, from 10.10.1.0 to 10.10.1.255)</li> </ul> </li> <li>Private Google Access: Turn it On (allows VMs without external IPs to access Google APIs)</li> <li>Flow logs: Turn Off (unless you need network monitoring)</li> </ul> <p></p> <ol> <li>Click DONE to finish subnet configuration  </li> <li>Click CREATE to create the VPC network</li> </ol>"},{"location":"migration/migration-create-vpc/#step-2-create-firewall-rules","title":"Step 2: Create Firewall Rules","text":"<p>Navigate to VPC network \u2192 Firewall and create these essential rules for your new VPC network:</p>"},{"location":"migration/migration-create-vpc/#rule-1-allow-ssh-access","title":"Rule 1: Allow SSH Access","text":"<ol> <li>Click Create Firewall Rule</li> <li>Configure the following settings:</li> <li>Name: <code>allow-ssh-ingress</code></li> <li>Description: <code>Allow SSH access from anywhere</code></li> <li>Network: <code>my-cloud-mastery-vpc</code></li> <li>Direction of traffic: Ingress</li> <li>Action on match: Allow</li> <li>Targets: All instances in the network</li> <li>Source filter: IPv4 ranges</li> <li>Source IPv4 ranges: <code>0.0.0.0/0</code></li> <li> <p>Protocols and ports: </p> <ul> <li>Check Specified protocols and ports</li> <li>Check TCP</li> <li>Enter port: <code>22</code></li> </ul> </li> <li> <p>Click CREATE</p> </li> </ol>"},{"location":"migration/migration-create-vpc/#rule-2-allow-http-traffic","title":"Rule 2: Allow HTTP Traffic","text":"<ol> <li>Click Create Firewall Rule</li> <li>Configure the following settings:</li> <li>Name: <code>allow-http-ingress</code></li> <li>Description: <code>Allow HTTP web traffic from anywhere</code></li> <li>Network: <code>my-cloud-mastery-vpc</code></li> <li>Direction of traffic: Ingress</li> <li>Action on match: Allow</li> <li>Targets: All instances in the network</li> <li>Source filter: IPv4 ranges</li> <li>Source IPv4 ranges: <code>0.0.0.0/0</code> (This allows web traffic from anyone on the internet)</li> <li>Protocols and ports:<ul> <li>Check Specified protocols and ports</li> <li>Check TCP</li> <li>Enter port: <code>80</code></li> </ul> </li> <li>Click CREATE </li> </ol>"},{"location":"migration/migration-create-vpc/#rule-3-allow-https-traffic-optional-but-recommended","title":"Rule 3: Allow HTTPS Traffic (Optional but Recommended)","text":"<ol> <li>Click Create Firewall Rule</li> <li>Configure the following settings:</li> <li>Name: <code>allow-https-ingress</code></li> <li>Description: <code>Allow HTTPS web traffic from anywhere</code></li> <li>Network: <code>my-cloud-mastery-vpc</code></li> <li>Direction of traffic: Ingress</li> <li>Action on match: Allow</li> <li>Targets: All instances in the network</li> <li>Source filter: IPv4 ranges</li> <li>Source IPv4 ranges: <code>0.0.0.0/0</code></li> <li> <p>Protocols and ports:</p> <ul> <li>Check Specified protocols and ports</li> <li>Check TCP</li> <li>Enter port: <code>443</code></li> </ul> </li> <li> <p>Click CREATE</p> </li> </ol>"},{"location":"migration/migration-create-vpc/#step-3-verify-your-vpc-configuration","title":"Step 3: Verify Your VPC Configuration","text":"<ol> <li>Navigate back to VPC networks</li> <li>Click on your newly created VPC (<code>my-cloud-mastery-vpc</code>)</li> <li>Verify the following:</li> <li>Subnets: Your subnet should be listed with the correct IP range</li> <li>Firewall rules: All three rules should be associated with your VPC</li> <li>Routes: Default routes should be automatically created</li> </ol>"},{"location":"migration/migration-create-vpc/#network-architecture-overview","title":"Network Architecture Overview","text":"<p>Your VPC now provides:</p> <ul> <li>Isolation: Your migrated application will run in its own network environment</li> <li>Security: Controlled access through firewall rules</li> <li>Scalability: Room for additional subnets and resources as needed</li> <li>Private Google Access: Ability to access Google APIs without external IPs</li> </ul> <p>VPC Setup Complete</p> <p>Your Google Cloud VPC is now ready to host your migrated virtual machine. The network provides proper isolation and security for your application.</p>"},{"location":"migration/migration-create-vpc/#whats-next","title":"What's Next","text":"<p>With your VPC network configured, you're ready to move to Phase 7 where we'll import your VMDK file as a bootable GCP image.</p> \u2190 Previous: Upload Image to GCS Section 7 -  Create GCP VPC  Next: Import Image \u2192"},{"location":"migration/migration-dns-update/","title":"Accessing your Migrated Application","text":"<p>The final step of the migration is to use your VM instance's external IP address to confirm that the application is up</p>"},{"location":"migration/migration-dns-update/#identify-your-new-ip-address","title":"Identify Your New IP Address","text":"<p>First, confirm the public IP address of your newly created VM.</p> <ol> <li>Navigate to the Compute Engine &gt; VM Instances page in the Google Cloud Console.</li> <li>Locate your VM (<code>pawait-vm-gcp-final</code>).</li> <li>Copy the value from the External IP column. This is your new server address.</li> </ol> <p>You can paste it on a new tab on a browser of your choice</p> <p></p> \u2190 Previous:  Access &amp; Configure VM Section 11 - Access via IP Next: DevOps Lab \u2192"},{"location":"migration/migration-gcloud-cli/","title":"Install Google Cloud CLI and Authenticate","text":"<p>In this phase, we'll install the Google Cloud CLI on your server and authenticate it to access your Google Cloud resources.</p>"},{"location":"migration/migration-gcloud-cli/#step-1-install-google-cloud-cli","title":"Step 1: Install Google Cloud CLI","text":"<p>Google Cloud CLI needs to be installed on the server. Follow the installation process based on your operating system.</p>"},{"location":"migration/migration-gcloud-cli/#for-ubuntudebian-systems","title":"For Ubuntu/Debian Systems","text":"<pre><code># Add the Cloud SDK distribution URI as a package source\necho \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n\n# Import the Google Cloud Platform public key\ncurl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n\n# Update the package list and install the Cloud SDK\nsudo apt-get update &amp;&amp; sudo apt-get install google-cloud-cli\n</code></pre>"},{"location":"migration/migration-gcloud-cli/#alternative-installation-methods","title":"Alternative Installation Methods","text":"<p>For other installation methods, refer to the official documentation: How to install the Google Cloud CLI</p>"},{"location":"migration/migration-gcloud-cli/#step-2-initialize-and-authenticate-google-cloud-cli","title":"Step 2: Initialize and Authenticate Google Cloud CLI","text":"<p>Run <code>gcloud init</code> to get started:</p> <p><pre><code>gcloud init\n</code></pre> </p>"},{"location":"migration/migration-gcloud-cli/#authentication-process","title":"Authentication Process","text":"<ol> <li>Choose Configuration: Select option <code>[1] Re-initialize this configuration [default] with new settings</code></li> <li> <p>Login: You'll be prompted to log in. Choose <code>Y</code> to continue with login   </p> </li> <li> <p>Browser Authentication</p> </li> </ol> <p>A URL will be provided for authentication</p> <p>Copy and paste the URL into your web browser</p> <p></p> <p>Log in with your Google Cloud account</p> <p></p> <p>Grant the necessary permissions</p> <p>   Copy the verification code from the browser    Enter Verification Code: Paste the verification code back in your terminal    If successful, you will see a message letting you know which user you have signed in as    Select Project: Choose the Google Cloud project where you want to perform the migration    </p> <p>Default Region/Zone: Configure your default compute region and zone (recommended: choose the same region where you created your storage bucket) You can also do this by running this command: </p> <p>Tip</p> <p>Replace zone-name with the us-central1-a</p> <pre><code>gcloud config set compute/zone zone-name\n</code></pre>"},{"location":"migration/migration-gcloud-cli/#verification","title":"Verification","text":"<p>After completing the authentication process, verify your setup:</p> <pre><code># Check your active account\ngcloud auth list\n\n# Check your active project\ngcloud config list project\n\n# Test access to your storage bucket\ngsutil ls gs://your-bucket-name\n</code></pre> <p>Authentication Complete</p> <p>You should see confirmation that you are signed in and your project is selected. You're now ready to upload files to Google Cloud Storage.</p>"},{"location":"migration/migration-gcloud-cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"migration/migration-gcloud-cli/#common-issues","title":"Common Issues","text":"<p>Issue: \"gcloud: command not found\"   Solution: Restart your terminal or source your shell profile: <pre><code>source ~/.bashrc # For Linux based systems\n# or\nsource ~/.zshrc # For MacOS based systems\n</code></pre></p> <p>Issue: Authentication timeout Solution: Ensure you complete the browser authentication within the time limit. If it expires, run <code>gcloud auth login</code> to retry.</p> <p>Issue: Permission denied errors Solution: Make sure your Google Cloud account has the necessary permissions for the project.</p> \u2190 Previous: Create GCS Bucket Section 5 - Install &amp; Configure gcloud CLI Next: Upload Image to GCS \u2192"},{"location":"migration/migration-gcs-bucket/","title":"Create a Google Cloud Storage Bucket","text":"<p>You can create a Google Cloud Storage bucket using either:</p> <ul> <li>Google Cloud Console (GUI): The easiest method for beginners, using a web browser</li> <li>Google Cloud CLI (gcloud): A faster method for those who prefer the command line or need to automate tasks</li> </ul>"},{"location":"migration/migration-gcs-bucket/#step-1-create-a-google-cloud-storage-bucket","title":"Step 1: Create a Google Cloud Storage Bucket","text":""},{"location":"migration/migration-gcs-bucket/#using-google-cloud-console-recommended","title":"Using Google Cloud Console (Recommended)","text":"<ol> <li> <p>Login to your Google Cloud account in the browser</p> </li> <li> <p>Navigate to Cloud Storage</p> </li> <li>On the top-left corner, click the Navigation menu (\u2630)</li> <li>Scroll down to the \"Storage\" section and click on Cloud Storage &gt; Buckets</li> </ol> <p>Pro Tip</p> <p>You can also type \"Buckets\" into the search bar at the top of the console and select the Cloud Storage Buckets page from the results.</p> <ol> <li>Start bucket creation</li> <li>On the \"Buckets\" page, click the \"+ CREATE\" or \"CREATE BUCKET\" button</li> <li> <p>This will open the bucket creation wizard</p> </li> <li> <p>Name your bucket</p> </li> <li>Give your bucket a globally unique name (e.g., <code>eddie-ngugi-cloud-mastery</code>)</li> <li>Naming rules: Must contain only lowercase letters, numbers, dashes (-), and underscores (_). Cannot begin or end with a dash</li> <li>Suggestion: Use your project ID or company name as a prefix for uniqueness (e.g., <code>mycompany-vm-image-storage</code>)</li> <li> <p>Click \"CONTINUE\"</p> </li> <li> <p>Choose where to store your data (Location)</p> </li> <li>Location type: Select \"Region\" (best choice for this migration task)<ul> <li>Multi-region: Highest availability, most expensive</li> <li>Dual-region: High availability across two regions</li> <li>Region: Single geographic location, best balance of cost and performance</li> </ul> </li> <li>Region: Select a region close to where you'll create your VM (e.g., us-central1 (Iowa))</li> <li> <p>Click \"CONTINUE\"</p> </li> <li> <p>Choose a default storage class</p> </li> <li>Select \"Standard\" - best choice for frequently accessed data</li> <li>The VMDK file will be accessed frequently during the import process</li> <li> <p>Click \"CONTINUE\"</p> </li> <li> <p>Choose how to control access to objects</p> </li> <li>Select \"Uniform\" (Enforce public access prevention is recommended)</li> <li>This ensures all access control is managed at the bucket level using IAM permissions</li> <li>This approach is simpler and more secure</li> <li> <p>Click \"CONTINUE\"</p> </li> <li> <p>Choose how to protect object data (Optional)</p> </li> <li>For this temporary migration task, leave these settings as \"None\"</li> <li>Advanced features like versioning and retention policies aren't needed</li> <li>Click \"CREATE\"</li> </ol>"},{"location":"migration/migration-gcs-bucket/#using-google-cloud-cli-alternative","title":"Using Google Cloud CLI (Alternative)","text":"<p>If you prefer using the command line, you can create a bucket with:</p> <pre><code>gsutil mb gs://your-project-migration-images\n</code></pre>"},{"location":"migration/migration-gcs-bucket/#step-2-verify-your-cloud-storage-bucket","title":"Step 2: Verify Your Cloud Storage Bucket","text":"<ol> <li>Navigate back to the Cloud Storage &gt; Buckets page</li> <li>Verify that your bucket appears in the list</li> <li>Click on your bucket name to view its contents (it should be empty)</li> <li>Note the bucket name as you'll need it in the next phases</li> </ol>"},{"location":"migration/migration-gcs-bucket/#bucket-configuration-best-practices","title":"Bucket Configuration Best Practices","text":"<p>Security Considerations</p> <ul> <li>Use uniform bucket-level access for better security management</li> <li>Consider enabling Object Versioning if you want to keep multiple versions of your images</li> <li>Set up appropriate IAM permissions for your team members</li> </ul>"},{"location":"migration/migration-gcs-bucket/#whats-next","title":"What's Next","text":"<p>Your Google Cloud Storage bucket is now ready to receive the VMDK file you created in Phase 2. In the next phase, we'll install and configure the Google Cloud CLI to authenticate and upload your disk image.</p> \u2190 Previous: Create VM Disk Image Section 4 - Create GCS Bucket Next: Install &amp; Configure gcloud CLI \u2192"},{"location":"migration/migration-import-image/","title":"Import the VMDK as a Bootable GCP Image","text":"<p>This is the most important step in the migration process. It converts your uploaded <code>.vmdk</code> disk file into a bootable image template that Google Cloud can use to create new virtual machines.</p>"},{"location":"migration/migration-import-image/#step-1-enable-the-vm-migration-api","title":"Step 1: Enable the VM Migration API","text":"<p>Before you can import an image, you must ensure the VM Migration API (also known as \"Migrate to Virtual Machines API\") is enabled for your project.</p> <ol> <li>Navigate to the VM Migration API page in the Google Cloud Console:     https://console.cloud.google.com/compute/mfce</li> <li>If the API is not already enabled, you will see an Enable button. Click it to activate the API. This may take a moment.</li> </ol> <p></p>"},{"location":"migration/migration-import-image/#step-2-set-your-target-project","title":"Step 2: Set your Target Project","text":"<ol> <li> <p>In the Google Cloud console, navigate to the Migrate to Virtual Machines section and select the Targets Tab</p> </li> <li> <p>Under the Targets Tab click on Add A Target project. </p> </li> <li>For this case select the current working project and click Add</li> </ol> <p> 4. Once done the project is added as part of the list of target projects. </p>"},{"location":"migration/migration-import-image/#step-3-navigate-to-the-image-imports-tab-and-click-create-image","title":"Step 3: Navigate to the Image Imports Tab  and Click Create Image","text":""},{"location":"migration/migration-import-image/#step-4-create-an-image","title":"Step 4 : Create an Image","text":"<p>Under the resulting page, enter the Name of the  Image, you would like to create i.e <code>cloud-mastery-image</code></p> <ul> <li>This will be the name of the final bootable image in GCP.</li> <li>Grant the <code>Storage Object User</code> IAM role to the VM Migration service account on the public bucket located at https://console.cloud.google.com/storage/browser/cloudmastery-image.</li> <li> <p>The service account name will be similar to: <code>service-10892896680@gcp-sa-vmmigration.iam.gserviceaccount.com</code>. Remember to copy it.     </p> </li> <li> <p>Source:</p> <ol> <li>Click BROWSE.</li> <li>Navigate to your Google Cloud Storage bucket (<code>cloudmastery-image</code>).</li> <li>Select the <code>.vmdk</code> file you uploaded earlier: <code>ubuntu-vm.vmdk</code>.</li> <li>Click SELECT.</li> </ol> </li> <li> <p>Region: <code>us-central1</code></p> <ul> <li>It's best practice to import the image into the same region where you plan to deploy the VM.</li> </ul> </li> <li> <p>Target project: <code>eddie-ngugi-pawait-1</code></p> <ul> <li>Ensure this is set to the correct project where you want the image to reside.</li> <li>For our case, its the same project we are working on.</li> </ul> </li> <li> <p>Family: <code>ubuntu-2204</code></p> <ul> <li>Grouping images into families makes them easier to manage.</li> </ul> </li> <li> <p>Licenses: Leave this as Default. GCP will automatically detect the OS.</p> </li> <li> <p>Advanced options (leave as default):</p> <ul> <li>Do not check \"Skip OS adaptation,\" \"Generalize,\" or \"Convert BIOS to UEFI\" unless you have a specific reason. The import tool handles these conversions well automatically.</li> </ul> </li> </ul> <p></p> <p>Click CREATE.</p> <p>Import in Progress</p> <p>The image import process will begin. This can take anywhere from 10 to 30 minutes, or longer, depending on the size of the disk image. You can monitor the progress on the Image imports tab.</p>"},{"location":"migration/migration-import-image/#step-5-validation","title":"Step 5: Validation","text":"<p>You newly imported image should be like so below</p> <p></p> <p>Image Import Complete</p> <p>Once the status shows as \"Succeeded,\" you have successfully converted your on-prem disk into a reusable, bootable image in Google Cloud.</p>"},{"location":"migration/migration-import-image/#whats-next","title":"What's Next","text":"<p>Now that you have a custom bootable image, you're ready for the final steps. In the next phase, you'll use this image to create a new VM instance in GCP.</p> \u2190 Previous: Create GCP VPC Section 8 - Import VMDK as GCP Image Next: Create GCP VM Instance \u2192"},{"location":"migration/migration-overview/","title":"Application Migration to Google Cloud Platform - Overview","text":"<p>This comprehensive guide will walk you through the process of creating an exact, bootable copy (a \"clone\") of a live on-premises Linux virtual machine (VM) and migrating it to a new, running VM in Google Cloud Platform (GCP).</p>"},{"location":"migration/migration-overview/#migration-strategy","title":"Migration Strategy","text":"<p>We will use a safe and reliable method that involves attaching a temporary \"helper\" disk to your on-premises VM. This ensures we don't accidentally fill up the main operating system disk, which could crash the machine.</p> <p>Analogy</p> <p>Instead of doing a messy construction project inside your kitchen while you're cooking, we're going to build everything in the garage (the temporary disk) and only move the finished product.</p>"},{"location":"migration/migration-overview/#migration-process-overview","title":"Migration Process Overview","text":"<p>The migration process consists of 10 phases:</p> <ol> <li>Phase 1: Preparing the On-Premises VM for Cloning</li> <li>Phase 2: Creating the Virtual Disk Image (VMDK)</li> <li>Phase 3: Create a Google Cloud Storage Bucket</li> <li>Phase 4: Install Google Cloud CLI and Authenticate</li> <li>Phase 5: Uploading the Image to Google Cloud</li> <li>Phase 6: Create a Google Cloud VPC</li> <li>Phase 7: Import the VMDK as a Bootable GCP Image</li> <li>Phase 8: Creating the New Virtual Machine</li> <li>Phase 9: Access the New Virtual Machine</li> <li>Phase 10: Change the DNS Records</li> </ol>"},{"location":"migration/migration-overview/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>On-Premises VM Access: You need to be able to log into your on-premises Linux VM via SSH and have sudo (administrator) privileges</li> <li>On-Premises Provider Access: You need the ability to contact your service provider to request that a new, temporary disk be attached to your VM if possible. For our case, this additional storage is allocated to the source virtual machine as an additional Local Disk</li> <li>Google Cloud Platform (GCP) Account: You need a GCP account with an active project and billing enabled</li> <li>A Google Cloud Storage (GCS) Bucket: You need a GCS bucket already created to store the disk image file</li> <li>SSH Client: You will need an SSH client on your local device to access the Linux Virtual Machine via SSH</li> </ul>"},{"location":"migration/migration-overview/#tools-required","title":"Tools Required","text":"<p>Throughout this migration, we will use several tools:</p> <ul> <li>SSH Client: We recommend Termius, which is a cross-platform SSH client and terminal emulator available for laptops, desktop, and mobile devices. You can learn how to install and set up Termius from here</li> <li>QEMU Utilities: For disk format conversion</li> <li>Google Cloud CLI: For authentication and file uploads</li> <li>Google Cloud Console: For managing GCP resources</li> </ul> <p>Important</p> <p>The disk imaging process (Phase 2) will take the longest time due to the size of the disk. Plan accordingly and ensure you have a stable connection throughout the process.</p>"},{"location":"migration/migration-overview/#what-youll-achieve","title":"What You'll Achieve","text":"<p>By the end of this migration guide, you will have:</p> <ul> <li>Successfully cloned your on-premises VM to Google Cloud Platform</li> <li>Created a properly configured GCP environment with VPC and firewall rules</li> <li>Deployed your application on GCP with the same functionality as the original</li> <li>Updated DNS records to point to the new GCP-hosted application</li> <li>A fully operational application running on Google Cloud infrastructure</li> </ul> \u2190 Previous: Introduction Section 1 - Application Migration to GCP Next: Prep On-Prem VM \u2192"},{"location":"migration/migration-prepare-vm/","title":"Preparing the On-Premises VM for Cloning","text":"<p>In this phase, we will prepare the source Virtual Machine to be copied safely with the installed application.</p>"},{"location":"migration/migration-prepare-vm/#step-1-log-into-the-source-on-premises-linux-vm-via-ssh","title":"Step 1: Log into the Source On-Premises Linux VM via SSH","text":"<p>Using your preferred SSH or FTP tool, access the VM using the provided IP for your VM and username and password. For this example, we will use Termius as the client.</p> <p>Example Credentials</p> <ul> <li>IP Address: 213.148.17.53</li> <li>Username: root</li> <li>Password: cloud-mastery</li> <li>Domain: train.cloudpartner.africa</li> </ul> <p>Open a Terminal using your preferred SSH client. We recommend Termius, which is a cross-platform SSH client and terminal emulator available for laptops, desktop, and mobile devices. You can learn how to install and set up Termius from https://termius.com/documentation/installation.</p> <p>For context, we are migrating the application hosted on this virtual machine and accessible via the IP 213.148.17.53 and mapped to http://train.cloudpartner.africa/</p> <p></p>"},{"location":"migration/migration-prepare-vm/#step-2-install-qemu-utilities","title":"Step 2: Install QEMU Utilities","text":"<p>This provides the <code>qemu-img</code> tool, which is essential for converting disk formats to our preferred <code>.vmdk</code> format to allow us to import the Image in an acceptable format.</p> <p><pre><code>sudo apt install update\nsudo apt-get install -y qemu-utils cloud-guest-utils\n</code></pre> </p>"},{"location":"migration/migration-prepare-vm/#step-3-identify-the-source-disk","title":"Step 3: Identify the Source Disk","text":"<p>Use <code>lsblk</code> to list block devices:</p> <p><pre><code>lsblk\n</code></pre> </p> <p>The original VM's disk will appear as a device, e.g., <code>/dev/sda</code>, <code>/dev/vda</code>, <code>/dev/nvme0n1</code>. Look for the disk that matches the size of your VM's disk.</p>"},{"location":"migration/migration-prepare-vm/#create-a-partition","title":"Create a partition","text":"<p><pre><code>sudo fdisk /dev/sdb\n</code></pre> Inside the fdisk prompt, press these keys in order, followed by Enter: n, p, 1, Enter, Enter, w.</p>"},{"location":"migration/migration-prepare-vm/#format-the-partition","title":"Format the partition","text":"<pre><code>sudo mkfs.ext4 /dev/sdb1\n</code></pre>"},{"location":"migration/migration-prepare-vm/#mount-the-partition","title":"Mount the partition","text":"<p><pre><code>sudo mkdir /mnt/imagedisk\nsudo mount /dev/sdb1 /mnt/imagedisk\n</code></pre> Expand the Filesystem (Crucial Step): If you expanded the disk after initially formatting it, you must resize the filesystem to use the new space.</p> <pre><code>sudo resize2fs /dev/sdb1\n</code></pre>"},{"location":"migration/migration-prepare-vm/#verify-check-that-the-disk-is-ready-and-shows-its-full-size","title":"Verify: Check that the disk is ready and shows its full size.","text":"<pre><code>df -h /mnt/imagedisk\n</code></pre>"},{"location":"migration/migration-prepare-vm/#step-4-identify-the-services-running-on-the-source-vm","title":"Step 4: Identify the Services Running on the Source VM","text":"<p>To identify the services that are running on the source VM, run the following command:</p> <p><pre><code>systemctl list-units --type=service --state=running\n</code></pre> </p>"},{"location":"migration/migration-prepare-vm/#step-5-stop-high-priority-applications-and-job-services","title":"Step 5: Stop High Priority Applications and Job Services","text":"<p>Important</p> <p>In order to create an image of the source disk, we need to stop all services running on the Virtual Machine.</p> <p>To perform this action, run the following commands:</p> <p><pre><code># Stop high-priority application and job services\nsudo systemctl stop docker.service\nsudo systemctl stop containerd.service\nsudo systemctl stop cron.service\nsudo systemctl stop atd.service\nsudo systemctl stop unattended-upgrades.service\n\n# Stop lower-priority logging and utility services\nsudo systemctl stop rsyslog.service\nsudo systemctl stop qemu-guest-agent.service\nsudo systemctl stop systemd-timesyncd.service\n\necho \"--- Services have been stopped. System is ready for imaging. ---\"\n</code></pre> </p>"},{"location":"migration/migration-prepare-vm/#step-6-force-all-cached-data-to-disk","title":"Step 6: Force All Cached Data to Disk","text":"<p>This command ensures that what's in memory gets written to the disk, making the clone more consistent:</p> <pre><code>sudo sync &amp;&amp; sudo sync\n</code></pre> <p>Phase 1 Complete</p> <p>Your VM is now prepared for cloning. All services have been stopped and cached data has been written to disk.</p> \u2190 Previous: Migration Overview Section 2 - Prepare On-Prem VM Next: Create VM Disk Image \u2192"},{"location":"migration/migration-upload-image/","title":"Uploading the Image to Google Cloud","text":"<p>In this phase, we'll upload the VMDK file you created in Phase 2 to your Google Cloud Storage bucket.</p>"},{"location":"migration/migration-upload-image/#step-1-set-your-active-project","title":"Step 1: Set Your Active Project","text":"<p>Ensure you're working with the correct Google Cloud project by setting it explicitly:</p> <pre><code>gcloud config set project your-project-id\n</code></pre> <p>Replace <code>your-project-id</code> with your actual project ID (e.g., <code>eddie-ngugi-pawait-1</code>).</p>"},{"location":"migration/migration-upload-image/#step-2-upload-the-vmdk-file-to-google-cloud-storage","title":"Step 2: Upload the VMDK File to Google Cloud Storage","text":"<p>Use the <code>gcloud storage cp</code> command to upload your VMDK file to the bucket you created in Phase 3:</p> <pre><code>gcloud storage cp /mnt/imagedisk/ubuntu-vm.vmdk gs://your-bucket-name/\n</code></pre> <p>Command Explanation:</p> <ul> <li><code>/mnt/imagedisk/ubuntu-vm.vmdk</code>: The local path to your VMDK file</li> <li><code>gs://your-bucket-name/</code>: Your Google Cloud Storage bucket URL</li> </ul>"},{"location":"migration/migration-upload-image/#alternative-upload-method","title":"Alternative Upload Method","text":"<p>You can also use the traditional <code>gsutil</code> command:</p> <pre><code>gsutil cp /mnt/imagedisk/ubuntu-vm.vmdk gs://your-bucket-name/\n</code></pre>"},{"location":"migration/migration-upload-image/#step-3-monitor-upload-progress","title":"Step 3: Monitor Upload Progress","text":"<p>The upload process will show progress. For large files, this may take considerable time depending on your internet connection speed.</p>"},{"location":"migration/migration-upload-image/#step-4-verify-the-upload","title":"Step 4: Verify the Upload","text":"<p>After the upload completes, verify that your file is in the bucket:</p> <pre><code># List all files in your bucket\ngsutil ls gs://your-bucket-name/\n\n# Get detailed information about your uploaded file\ngsutil ls -l gs://your-bucket-name/ubuntu-vm.vmdk\n</code></pre> <p>You should see your <code>ubuntu-vm.vmdk</code> file listed with its size and upload timestamp.</p>"},{"location":"migration/migration-upload-image/#step-5-verify-via-google-cloud-console","title":"Step 5: Verify via Google Cloud Console","text":"<p>You can also verify the upload through the web interface:</p> <ol> <li>Go to the Google Cloud Console</li> <li>Navigate to Cloud Storage &gt; Buckets</li> <li>Click on your bucket name</li> <li>Verify that <code>ubuntu-vm.vmdk</code> appears in the file list</li> </ol>"},{"location":"migration/migration-upload-image/#troubleshooting-upload-issues","title":"Troubleshooting Upload Issues","text":""},{"location":"migration/migration-upload-image/#common-problems-and-solutions","title":"Common Problems and Solutions","text":"<p>Issue: Upload fails with authentication errors Solution: Re-authenticate with <code>gcloud auth login</code></p> <p>Issue: Upload is very slow Solution:  - Check your internet connection - Consider using <code>gsutil -m cp</code> for parallel uploads of multiple files - Ensure no other heavy network activities are running</p> <p>Issue: \"Insufficient permissions\" error Solution: Ensure your account has Storage Admin or Storage Object Admin role for the bucket</p> <p>Issue: Upload stops/fails partway through Solution: The <code>gsutil cp</code> command is resumable - simply run the same command again to resume the upload</p>"},{"location":"migration/migration-upload-image/#whats-next","title":"What's Next","text":"<p>Once your VMDK file is successfully uploaded to Google Cloud Storage, you're ready to move to Phase 6 where we'll create a VPC network for your migrated application.</p> <p>Upload Complete</p> <p>Your VM disk image is now safely stored in Google Cloud Storage and ready for the next phase of migration.</p> \u2190 Previous: Install &amp; Configure gcloud CLI Section 6 -  Upload Image to GCS Next: Create GCP VPC \u2192"},{"location":"virtual-agent/virtual-agent-lab/","title":"Building conversational agents with DialogflowCX","text":"<p>This agent enables customers to browse e-commerce products and complete purchases</p>"},{"location":"virtual-agent/virtual-agent-lab/#preparing-your-agent-environment","title":"Preparing your agent environment","text":"<ol> <li>Log into Conversational Agents</li> <li>Select your project. It is already preloaded with your files. You can also  download the files here</li> <li>Click the Create Agent button<ol> <li>Choose \"Build Your Own\" as the agent type</li> <li>Enter the agent name and select \"Global\" for the region</li> <li>Set the timezone to +3</li> <li>Select a Playbook</li> <li>Done</li> </ol> </li> <li>You might also need to enable GCP APIs          </li> </ol>"},{"location":"virtual-agent/virtual-agent-lab/#step-wise-execution-the-mental-model","title":"Step Wise Execution : The Mental Model","text":"<ol> <li> <p>This is just a clear way that man and AI can communicate with clarity</p> <ul> <li>We want to set goals, not instructions</li> <li>Clarity is the key to success here because you are dealing with very high IQ here</li> <li>We need focused queries to get best value from the AI model (LLM)</li> <li> <p>Here is some perspective.</p> <p></p> </li> </ul> </li> <li> <p>A word for the wise (we used AI to generate this wisdom):</p> <p></p> </li> <li> <p>Our mental model will be Step Wise Execution. </p> <ol> <li>Each big task will be broken into small chunks. </li> <li>We shall take time to explain everything to the AI like a child. </li> <li>Clarity and verbosity is the best simplicity</li> <li>How does this translate? Example steps to collect a order:</li> </ol> </li> </ol> <pre><code>- Step 1. Do not greet the user, directly start to collect their order information.\n    - Step 1.1. Ask the customer to provide their full names.\n    - Step 1.2. Ask the customer to provide their mobile number.\n        - Step 1.2.1. Explain it will be given to the delivery guy for the purposes of reaching you in regards to the delivery only.\n    - Step 1.3. Ask the customer to provide their delivery address including street name, building name and house number.\n    - Step 1.4. Ask the customer for the best time to deliver the battery.\n- Step 2. Ask the customer to confirm their order information from Step 1.\n    - Step 2.1. To maintain clarity, use bullet points to lay out the confirmation information.\n- Step 3. Send payment information to the customer to pay for the battery.\n    - Step 3.1. To maintain clarity, use bullet points to lay out the payment information.\n    - Step 3.2. Payment will be done with the following MPESA details:\n        - MPESA Paybill Number 123456\n        - Account Number: 000111\n        - Amount: check product_data parameter and display the price\n    - Step 3.3. Tell the user an agent will call them within 30 mins of confirmed payment\n        - Step 3.3.1. Tell them the delivery guy will come in a branded Chloride Exide van.\n        - Step 3.3.2. Ask them to be available at the agreed delivery time to facilitate smooth delivery.\n    - Step 3.4. Provide the user with a summary of their order\n        - Step 3.4.1. Include relevant information from product_data parameter.\n        - Step 3.4.2. To maintain clarity, use bullet points to lay out the payment information.\n    - Step 3.5. Ask the user if there is anything else they need help with another car.\n        - Step 3.5.1 If the user needs battery for another car, start again from the beginning\n        - Step 3.5.2. If the user does not need any more help end the conversation using ${PLAYBOOK:Bye Task Playbook}\n</code></pre>"},{"location":"virtual-agent/virtual-agent-lab/#workflow-image","title":"Workflow Image","text":"<p>We shall approach the agent build in reverse because these assistants fully depend on each other. The order of work shall be:</p> <ol> <li>Build data access tool</li> <li>Build Bye Assistant</li> <li>Building Place Order Assistant</li> <li>Building the Inventory Assistant</li> <li>Building the Steering Assistant</li> <li>Debugging and showcase</li> </ol> <p></p>"},{"location":"virtual-agent/virtual-agent-lab/#build-the-product-data-tool","title":"Build the Product Data Tool","text":"<p>This tool provides access to product data. </p> <ol> <li>Under Tools click Create</li> <li>Tool Name: <code>ecommerce_data</code></li> <li>Type: <code>Data store</code></li> <li>Description: <code>Contains information about  ecommerce products</code></li> <li>Add Datastore from GCS bucket</li> <li>Select Datastore</li> <li>Enter your company name</li> <li>Model: <code>gemini-2.0-flash-001</code></li> <li>Retain default prompt</li> <li>Save </li> </ol>"},{"location":"virtual-agent/virtual-agent-lab/#building-bye-assistant","title":"Building Bye Assistant","text":"<p>This assistant says bye bye to the customer.</p> <p>Goal:</p> <pre><code>To end the conversation with the user politely\n</code></pre> <p>Instructions:</p> <pre><code>- Step 1. Thank the user for choosing to buy from us.\n- Step 2. Inform them they will receive their product soon.\n- Step 3. Make a funny joke about product_data and end the conversation.\n</code></pre>"},{"location":"virtual-agent/virtual-agent-lab/#building-place-order-assistant","title":"Building Place Order Assistant","text":"<p>This assistant collects customer details and places an order for them.</p> <p>Goal:</p> <pre><code>To collect necessary billing information from the customer and place the order for the customer if everything is confirmed.\n</code></pre> <p>Instructions:</p> <pre><code>- Step 1. Do not greet the user, directly start to collect their order information.\n    - Step 1.1. Ask the customer to provide their full names.\n    - Step 1.2. Ask the customer to provide their mobile number.\n        - Step 1.2.1. Explain it will be given to the delivery guy for the purposes of reaching you in regards to the delivery only.\n    - Step 1.3. Ask the customer to provide their delivery address including street name, building name and house number.\n    - Step 1.4. Ask the customer for the best time to deliver the order.\n- Step 2. Ask the customer to confirm their order information from Step 1.\n    - Step 2.1. To maintain clarity, use bullet points to lay out the confirmation information.\n- Step 3. Send payment information to the customer to pay for the order.\n    - Step 3.1. To maintain clarity, use bullet points to lay out the payment information.\n    - Step 3.2. Payment will be done with the following MPESA details:\n        - MPESA Paybill Number 123456\n        - Account Number: 000111\n        - Amount: check product_data parameter and display the price\n    - Step 3.3. Tell the user an agent will call them within 30 mins of confirmed payment\n        - Step 3.3.2. Ask them to be available at the agreed delivery time to facilitate smooth delivery.\n    - Step 3.4. Provide the user with a summary of their order\n        - Step 3.4.1. Include relevant information from product_data parameter.\n        - Step 3.4.2. To maintain clarity, use bullet points to lay out the payment information.\n    - Step 3.5. Ask the user if there is anything else they need help with.\n        - Step 3.5.1 If the user needs another product, start again from the beginning\n        - Step 3.5.2. If the user does not need any more help redirect to ${PLAYBOOK:Bye Assistant}\n</code></pre> <p>Parameters:</p> <pre><code>Input Params\n==================\nKey: product_data\nValue: Product selected by user\n\nOutput Params\n==================\nKey: product_data\nValue: Product selected by user\n</code></pre>"},{"location":"virtual-agent/virtual-agent-lab/#building-the-inventory-assistant","title":"Building the Inventory Assistant","text":"<p>This assistant has access to product data via a tool. it can answer questions about products, recommend items to the customer and cooperate with other agents to achieve shared goals.</p> <p>Goal:</p> <pre><code>To answer any questions about products\n</code></pre> <p>Instructions:</p> <pre><code>- Step 1: Ask the user which product the want to buy.\n    - Step 1.1: If the user has no idea what they want, list for them all the products in stock using ${TOOL:ecommerce_data}.\n        - Step 1.1.1. Let the user know you can recommend a product at any time if they ask.\n    - Step 1.2. If the user wants a recommendation, recommend at most 3 products.\n    - Step 1.3. After listing, ask the user what product they would like to buy.\n- Step 2. If the user wants a specific product, search for their product in ${TOOL:ecommerce_data}\n    - Step 2.1. When performing your search, generate the best query that describes the user's needs.\n    - Step 2.2. Lay out your response in the best and most efficient way possible.\n    - Step 2.3. All prices are in KES. When writing numbers, always place the tousandth comma e.g. KES 6,000\n- Step 3. If user has selected a product, confirm the user selection before proceeding to next step.\n- Step 4. If the user wants to order the product, send them onto ${PLAYBOOK:Place Order Playbook}\n- Step 5. If user does not want to order a product, redirect to ${PLAYBOOK:Bye Assistant}\n- When listing products, always use bullet points to ensure easy readability. Always include the price.\n</code></pre> <p>Datastore: <code>ecommerce_data</code></p> <p>Parameters:</p> <pre><code>Input Params\n==================\nKey: customer_name\nValue: Customer name\n\nOutput Params\n==================\nKey: product_data\nValue: Product selected by user\n</code></pre>"},{"location":"virtual-agent/virtual-agent-lab/#building-the-steering-assistant","title":"Building the Steering Assistant","text":"<p>This assistant will be like traffic police, responsible for guiding the conversation at high level (without taking bribes \ud83d\ude0a)</p> <p>Goal</p> <pre><code>To greet the user politely, state your purpose and ask how you can help them, then redirect them to another agent who will fulfil their request\n</code></pre> <p>Instructions</p> <pre><code>- Step 1. Greet the customer politely. Always say your name and purpose to the user.\n    - Your name is June. Always identify by this name.\n    - You work for Mali Safi Ltd.\n    - Your purpose is to delightfully assist the user find their dream product and place an order for it.\n- Step 2. Ask the user their name.\n- Step 3. Then redirect to ${PLAYBOOK:Inventory Assistant}\n</code></pre> <p>Other Connections</p> <pre><code>None\n</code></pre>"},{"location":"virtual-agent/virtual-agent-lab/#deploying-your-agent","title":"Deploying your agent","text":"<ul> <li>On the top navigation, click on Publish Agent</li> <li>Choose your deployment environment, access and UI style</li> <li>Click Enable Conversational Agent</li> <li>Copy the embed code displayed into your HTML template here</li> </ul> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"UTF-8\" /&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n    &lt;title&gt;Virtual Agent Class&lt;/title&gt;\n    &lt;!-- Include Tailwind CSS via CDN --&gt;\n    &lt;script src=\"https://cdn.tailwindcss.com\"&gt;&lt;/script&gt;\n\n    &lt;!-- PLACE YOUR AGENT EMBED CODE HERE --&gt;\n\n  &lt;/head&gt;\n  &lt;body class=\"m-0 p-0\"&gt;\n    &lt;!-- Main container with 100vh height --&gt;\n    &lt;div class=\"h-screen w-full flex flex-col bg-gray-100\"&gt;\n      &lt;!-- Header Section --&gt;\n      &lt;img\n        src=\"https://storage.googleapis.com/cm_ecommerce/image.png\"\n        class=\"w-full\"\n      /&gt;\n    &lt;/div&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre> \u2190 Previous: Visualize Data  Section 25 -  Virtual Agent Lab  Next: End of Training \u2192"}]}